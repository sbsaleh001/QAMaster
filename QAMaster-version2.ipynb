{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6828c2b",
   "metadata": {},
   "source": [
    "# QAMaster: Fine-tuned Q/A Chat Box\n",
    "- Version 2\n",
    "- Summer 2024\n",
    "\n",
    "This project creates a Q/A chat box using multiple pre-trained models. It fine-tunes these models on a Q/A dataset and evaluates them to choose the best one based on F1-score and Exact Match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b577276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64d7f26",
   "metadata": {},
   "source": [
    "## Intro\n",
    "- Loading models:\n",
    "      - Tuner 007\n",
    "      - GPT2\n",
    "- Generating sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "311464ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_finetune = [\n",
    "    \"tuner007/pegasus_paraphrase\",  # Scientific Research\n",
    "    \"gpt2\",                         # GPT-2\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72ca9e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForCausalLM were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_models = {}\n",
    "for model_name in models_to_finetune:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, config=config)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    tokenizer_models[model_name] = (tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b19aa077",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"input\": \"Hello, how can I assist you today?\", \"target\": \"Hi, what can I help you with today?\"},\n",
    "    {\"input\": \"What is the weather like?\", \"target\": \"Can you tell me what the weather is like?\"},\n",
    "    {\"input\": \"Tell me a joke.\", \"target\": \"Do you know any good jokes to tell me?\"},\n",
    "    {\"input\": \"Goodbye!\", \"target\": \"See you later! Have a great day!\"},\n",
    "    {\"input\": \"Can you help me with my homework?\", \"target\": \"Sure, I'd be happy to help with your homework!\"},\n",
    "    {\"input\": \"What's the capital of France?\", \"target\": \"The capital of France is Paris.\"},\n",
    "    {\"input\": \"How do I bake a cake?\", \"target\": \"To bake a cake, you need flour, sugar, eggs, and butter. Preheat your oven to 350 degrees.\"},\n",
    "    {\"input\": \"What's your name?\", \"target\": \"I am a chatbot created to assist you.\"},\n",
    "    {\"input\": \"Can you play music?\", \"target\": \"I can recommend some good music for you.\"},\n",
    "    {\"input\": \"What time is it?\", \"target\": \"I don't have access to real-time data, but you can check the time on your device.\"},\n",
    "    {\"input\": \"How can I help you?\", \"target\": \"What do you need help with today?\"},\n",
    "    {\"input\": \"What's the weather today?\", \"target\": \"Can you tell me about today's weather?\"},\n",
    "    {\"input\": \"Make me laugh.\", \"target\": \"Do you have any good jokes?\"},\n",
    "    {\"input\": \"Bye!\", \"target\": \"Goodbye! Have a great day!\"},\n",
    "    {\"input\": \"Can you assist me with my homework?\", \"target\": \"Yes, I can help you with your homework.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c051db2d",
   "metadata": {},
   "source": [
    "## PreProcessing\n",
    "- Split Dataset into Train and Test Sets\n",
    "- Tokenizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be1f4f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2e31353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data, tokenizer):\n",
    "    inputs = [example['input'] for example in data]\n",
    "    targets = [example['target'] for example in data]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True, padding=True)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    model_inputs['attention_mask'] = model_inputs['attention_mask']\n",
    "    return Dataset.from_dict(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36567763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/salehbabaei/opt/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tokenize_data(train_data, tokenizer_models['gpt2'][0])\n",
    "test_dataset = tokenize_data(test_data, tokenizer_models['gpt2'][0])\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e534b91",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "- Fine tuning funcion defined\n",
    "- Fine tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0aebf898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(tokenizer, model, train_dataset):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "    trainer.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "579367b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model_name, (tokenizer, model) in tokenizer_models.items():\n",
    "    fine_tuned_model = fine_tune_model(tokenizer, model, datasets['train'])\n",
    "    tokenizer_models[model_name] = (tokenizer, fine_tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57fc1b1",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "- F1-score\n",
    "- BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3d557df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(tokenizer, model, test_data):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for example in test_data:\n",
    "        inputs = tokenizer.encode(example[\"input\"], return_tensors=\"pt\")\n",
    "        attention_mask = tokenizer.batch_encode_plus([example[\"input\"]], padding=True, return_tensors=\"pt\")[\"attention_mask\"]\n",
    "        outputs = model.generate(inputs, max_length=50, attention_mask=attention_mask, pad_token_id=tokenizer.pad_token_id)\n",
    "        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(example[\"target\"])\n",
    "    \n",
    "    f1 = f1_score(references, predictions, average='weighted')\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc79c8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Scores: {'tuner007/pegasus_paraphrase': 0.0, 'gpt2': 0.0}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for model_name, (tokenizer, model) in tokenizer_models.items():\n",
    "    f1 = evaluate_model(tokenizer, model, test_data)\n",
    "    results[model_name] = f1\n",
    "\n",
    "print(\"F1 Scores:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71d76942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(predictions, references):\n",
    "    scores = []\n",
    "    smoothing_function = SmoothingFunction().method4\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        scores.append(sentence_bleu([ref.split()], pred.split(), smoothing_function=smoothing_function))\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "def evaluate_model_with_bleu(tokenizer, model, test_data):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for example in test_data:\n",
    "        inputs = tokenizer.encode(example[\"input\"], return_tensors=\"pt\")\n",
    "        attention_mask = tokenizer.batch_encode_plus([example[\"input\"]], padding=True, return_tensors=\"pt\")[\"attention_mask\"]\n",
    "        outputs = model.generate(inputs, max_length=50, attention_mask=attention_mask, pad_token_id=tokenizer.pad_token_id)\n",
    "        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predictions.append(decoded_output)\n",
    "        references.append(example[\"target\"])\n",
    "    \n",
    "    bleu = calculate_bleu(predictions, references)\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd936ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Scores: {'tuner007/pegasus_paraphrase': 0.027215891051044813, 'gpt2': 0.017204779418087093}\n"
     ]
    }
   ],
   "source": [
    "bleu_results = {}\n",
    "for model_name, (tokenizer, model) in tokenizer_models.items():\n",
    "    bleu = evaluate_model_with_bleu(tokenizer, model, test_data)\n",
    "    bleu_results[model_name] = bleu\n",
    "\n",
    "print(\"BLEU Scores:\", bleu_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf99206",
   "metadata": {},
   "source": [
    "## Chat box\n",
    "- Takes a question as input and answers with different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c8c358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, tokenizer, model):\n",
    "    inputs = tokenizer.encode(question, return_tensors=\"pt\")\n",
    "    attention_mask = tokenizer.batch_encode_plus([question], padding=True, return_tensors=\"pt\")[\"attention_mask\"]\n",
    "    outputs = model.generate(inputs, max_length=50, attention_mask=attention_mask, pad_token_id=tokenizer.pad_token_id)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "deb59164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask: Hi\n",
      "Answer from tuner007/pegasus_paraphrase: Hi,,.......................................................................................................................................\n",
      "Answer from gpt2: Hi, I'm not going to tell you how to do this. I'm not going to tell you how to do this. I'm not going to tell you how to do this. I'm not going to tell you how to do this.\n"
     ]
    }
   ],
   "source": [
    "question = input('Ask: ')\n",
    "for model_name, (tokenizer, model) in tokenizer_models.items():\n",
    "    answer = answer_question(question, tokenizer, model)\n",
    "    print(f\"Answer from {model_name}: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
