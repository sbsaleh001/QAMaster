{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e8560b",
   "metadata": {},
   "source": [
    "# QAMaster: Fine-tuned Q/A Chat Box\n",
    "- Summer 2024\n",
    "\n",
    "This project creates a Q/A chat box using multiple pre-trained models. It fine-tunes these models on a Q/A dataset and evaluates them to choose the best one based on F1-score and Exact Match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a6d3c0",
   "metadata": {},
   "source": [
    "hf access token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac72d432",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7a6f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, AutoConfig\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bcdc4e",
   "metadata": {},
   "source": [
    "## Intro\n",
    "- Loading Pre-trained Models\n",
    "- Loading Squad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef296f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aab91bbe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1407dd5771ed4613857a47f65219b7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/86.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/salehbabaei/opt/anaconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f92189983b44008c2aaf7eeeff2d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962470b7ef9c4c05a53beb466fa56b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69d2d5b1ea4477d9c5ca512a9ebd4cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sg/07yql_0n00vfn3gr8mgypndm0000gn/T/ipykernel_48763/4159807510.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels_to_finetune\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Use the token to authenticate and download models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2108\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2110\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   2111\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2112\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2334\u001b[0m         \u001b[0;31m# Instantiate the tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2335\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2336\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2337\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2338\u001b[0m             raise OSError(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/pegasus/tokenization_pegasus_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, tokenizer_file, pad_token, eos_token, unk_token, mask_token, mask_token_sent, additional_special_tokens, offset, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"added_tokens_decoder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mtokenizer_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mfast_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_slow_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslow_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0;34m\"Couldn't instantiate the backend tokenizer from one of: \\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;34m\"(1) a `tokenizers` library serialization file, \\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "models_to_finetune = [\n",
    "    \"Qwen/Qwen1.5-72B\",     # QWEn 1.5.72B\n",
    "    \"tuner007/pegasus_paraphrase\",  # Scientific Research\n",
    "    \"gpt2\",                 # gpt2\n",
    "    \"deepset/roberta-base-squad2\",  # Example model 1 (RoBERTa fine-tuned on SQuAD 2.0)\n",
    "    \"neuralmind/bert-large-portuguese-cased\"  # Example model 2 (BERT large for Portuguese)\n",
    "]\n",
    "\n",
    "tokenizer_models = {}\n",
    "for model_name in models_to_finetune:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n",
    "    config = AutoConfig.from_pretrained(model_name, use_auth_token=token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, config=config, use_auth_token=token)\n",
    "    \n",
    "    tokenizer_models[model_name] = (tokenizer, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b17c9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qa_dataset():\n",
    "    dataset = load_dataset(\"squad\")\n",
    "    return dataset\n",
    "dataset = load_qa_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092a3ada",
   "metadata": {},
   "source": [
    "## PreProcessing\n",
    "- Split QA Dataset into Train, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5068719",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"validation\"]\n",
    "\n",
    "train_dataset, eval_dataset = train_test_split(train_valid_dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622fd1f1",
   "metadata": {},
   "source": [
    "## Fine-tuning Function\n",
    "- Fine-tuning function defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1467121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model_name, tokenizer, model, train_dataset, eval_dataset, training_args):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a53035",
   "metadata": {},
   "source": [
    "## Fine-tune and Evaluation\n",
    "- Training arguments\n",
    "- Results\n",
    "- Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b37ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=12,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9891bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for model_name, (tokenizer, model) in tokenizer_models.items():\n",
    "    print(f\"Fine-tuning {model_name}...\")\n",
    "    eval_results = fine_tune_model(model_name, tokenizer, model, train_dataset, eval_dataset, training_args)\n",
    "    results[model_name] = eval_results\n",
    "\n",
    "    print(f\"Evaluation results for {model_name}:\")\n",
    "    print(f\"Exact Match: {eval_results['exact_match']:.2f}\")\n",
    "    print(f\"F1 Score: {eval_results['f1']:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037ea417",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = max(results, key=lambda x: results[x]['f1'])\n",
    "print(f\"Best model based on F1 score: {best_model}\")\n",
    "print(f\"Evaluation results - Exact Match: {results[best_model]['exact_match']:.2f}, F1 Score: {results[best_model]['f1']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
