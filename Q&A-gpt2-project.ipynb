{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf299eb9",
   "metadata": {},
   "source": [
    "# Q/A GPT2 \n",
    "\n",
    "June 13, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9838b1a",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aba3f3",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b0cb1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 17:59:34.994071: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a9006",
   "metadata": {},
   "source": [
    "Loading pre-trained GPT2 model and its tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c42673",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5fd404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2308b5",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadfa552",
   "metadata": {},
   "source": [
    "Preprocess function to create input-output pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fc93bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    examples = []\n",
    "    for item in data:\n",
    "        question = item[\"question\"]\n",
    "        answer = item[\"answer\"]\n",
    "        prompt = f\"Q: {question}\\nA: {answer}\\n\"\n",
    "        examples.append(prompt)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9888b328",
   "metadata": {},
   "source": [
    "fine-tuning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec08daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"The capital of France is Paris.\"},\n",
    "    {\"question\": \"Who wrote 'Pride and Prejudice'?\", \"answer\": \"Jane Austen wrote 'Pride and Prejudice'.\"},\n",
    "    {\"question\": \"What is the tallest mountain in the world?\", \"answer\": \"The tallest mountain in the world is Mount Everest.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8cd056f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q: What is the capital of France?\\nA: The capital of France is Paris.\\n',\n",
       " \"Q: Who wrote 'Pride and Prejudice'?\\nA: Jane Austen wrote 'Pride and Prejudice'.\\n\",\n",
       " 'Q: What is the tallest mountain in the world?\\nA: The tallest mountain in the world is Mount Everest.\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data = preprocess_data(data)\n",
    "processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e406b3",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "480fc17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b746b",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1280408",
   "metadata": {},
   "source": [
    "Create a dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1aa8b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict({\"text\": processed_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d8c44a",
   "metadata": {},
   "source": [
    "Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ca11f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a167d8d0c3f34a00813f55fc29c52d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc51f1e5",
   "metadata": {},
   "source": [
    "Define data collator for language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62ec2982",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9505cd6b",
   "metadata": {},
   "source": [
    "Function to generate a response from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f2d124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, question):\n",
    "    inputs = tokenizer.encode(f\"Q: {question}\\nA:\", return_tensors=\"pt\", padding=True)\n",
    "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_length=50, \n",
    "        num_return_sequences=1, \n",
    "        attention_mask=attention_mask, \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response.split(\"\\nA:\")[1].split(\"\\nQ:\")[0].strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10541e8",
   "metadata": {},
   "source": [
    "### Testing the model before fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "183f0e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who wrote 'Pride and Prejudice'?\",\n",
    "    \"What is the tallest mountain in the world?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cee1b368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses before fine-tuning:\n",
      "Question: What is the capital of France?\n",
      "Response: The capital of France is Paris.\n",
      "\n",
      "Question: Who wrote 'Pride and Prejudice'?\n",
      "Response: I think it was the author of the book, and I think it was the author of the book. I think it was the author of the book. I think it was\n",
      "\n",
      "Question: What is the tallest mountain in the world?\n",
      "Response: The tallest mountain in the world is the Himalayas. It is the tallest mountain in the world. It is the tallest mountain in the world. It is the tallest mountain in\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Responses before fine-tuning:\")\n",
    "for question in questions:\n",
    "    response = generate_response(model, tokenizer, question)\n",
    "    print(f\"Question: {question}\\nResponse: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f330db",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c5c2a0",
   "metadata": {},
   "source": [
    "**Set up training arguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de1c0f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb30c9a",
   "metadata": {},
   "source": [
    "Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "863a9944",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4155556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=1.4621446927388508, metrics={'train_runtime': 33.0695, 'train_samples_per_second': 0.272, 'train_steps_per_second': 0.181, 'total_flos': 2351628288000.0, 'train_loss': 1.4621446927388508, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfb776c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_model/tokenizer_config.json',\n",
       " './fine_tuned_model/special_tokens_map.json',\n",
       " './fine_tuned_model/vocab.json',\n",
       " './fine_tuned_model/merges.txt',\n",
       " './fine_tuned_model/added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81294d6",
   "metadata": {},
   "source": [
    "### Testing the model after fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71616f7",
   "metadata": {},
   "source": [
    "Load the fine-tuned model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4c9c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_model\")\n",
    "fine_tuned_tokenizer = GPT2Tokenizer.from_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "217649da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses before fine-tuning:\n",
      "Question: What is the capital of France?\n",
      "Response: The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of France is Paris. The capital of\n",
      "\n",
      "Question: Who wrote 'Pride and Prejudice'?\n",
      "Response: I wrote 'Pride and Prejudice' in the first place.\n",
      "\n",
      "Question: What is the tallest mountain in the world?\n",
      "Response: The tallest mountain in the world is Mount Everest.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Responses before fine-tuning:\")\n",
    "for question in questions:\n",
    "    response = generate_response(model, tokenizer, question)\n",
    "    print(f\"Question: {question}\\nResponse: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ea297b",
   "metadata": {},
   "source": [
    "# Part 2 - Fixing the looping problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a13cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response2(model, tokenizer, question):\n",
    "    inputs = tokenizer.encode(f\"Q: {question}\\nA:\", return_tensors=\"pt\", padding=True)\n",
    "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_length=50, \n",
    "        num_return_sequences=1, \n",
    "        attention_mask=attention_mask, \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=2,  # Prevents repetition of 2-grams\n",
    "        top_p=0.95,  # Uses nucleus sampling\n",
    "        top_k=50  # Limits the number of tokens to consider at each step\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract the part after \"A:\" and before any new question part if present\n",
    "    response = response.split(\"A:\")[1].split(\"Q:\")[0].strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27e8060e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses after fine-tuning:\n",
      "Question: What is the capital of France?\n",
      "Response: France is the capital of France is France.\n",
      "\n",
      "Question: Who wrote 'Pride and Prejudice'?\n",
      "Response: I wrote 'Pride and Prejudice' in the early 1970s.\n",
      "\n",
      "Question: What is the tallest mountain in the world?\n",
      "Response: The tallest mountain in the world is Mount Everest.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Responses after fine-tuning:\")\n",
    "for question in questions:\n",
    "    response = generate_response(model, tokenizer, question)\n",
    "    print(f\"Question: {question}\\nResponse: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0bfdf5",
   "metadata": {},
   "source": [
    "# models\n",
    "- QWEn 1.5.72B\n",
    "- Scientific Research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb2d88",
   "metadata": {},
   "source": [
    "# Concepts\n",
    "- domain specific Q/A models\n",
    "- nlp article type Q/As\n",
    "- how to tune weights in fine-tuning process\n",
    "- F1-score, exact match,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208bbbef",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "- Try 5 different models and evalute them **(QWEn 1.5.72B - Scientific Research)**\n",
    "- Upload the code on github and share with Samira\n",
    "- (optional) extract question and answers from articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0240ab22",
   "metadata": {},
   "source": [
    "# Deadline\n",
    "- Tuesday or Thursday this week(16-22)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
